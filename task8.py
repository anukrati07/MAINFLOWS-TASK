# -*- coding: utf-8 -*-
"""TASK8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14WqswYuwgUEWfcgurw1iVWtI4YkgLCYF

Section 1: Feature Engineering & Model Tuning
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder


# Step 1: Create Synthetic Dataset
def create_dataset(n_samples=500):
    np.random.seed(42)
    data = {
        'Math': np.random.randint(40, 100, n_samples),
        'Science': np.random.randint(40, 100, n_samples),
        'English': np.random.randint(40, 100, n_samples),
        'History': np.random.randint(40, 100, n_samples),
        'Pass': np.random.choice([0, 1], n_samples)
    }
    df = pd.DataFrame(data)
    return df

# Step 2: Feature Engineering
def feature_engineering(df):
    df['Total_Score'] = df[['Math', 'Science', 'English', 'History']].sum(axis=1)
    df['Average_Score'] = df['Total_Score'] / 4
    return df

# Step 3: Prepare Data for Model Training
def prepare_data(df):
    X = df.drop(columns=['Pass'])
    y = df['Pass']
    return train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Hyperparameter Tuning with GridSearchCV
def hyperparameter_tuning(X_train, y_train):
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5, 10]
    }
    model = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    return grid_search.best_estimator_

# Step 5: Train and Evaluate Model
def train_and_evaluate():
    df = create_dataset()
    df = feature_engineering(df)
    X_train, X_test, y_train, y_test = prepare_data(df)
    best_model = hyperparameter_tuning(X_train, y_train)
    y_pred = best_model.predict(X_test)
    print(f"Best Model: {best_model}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

if __name__ == "__main__":
    train_and_evaluate()

"""Section 2: Fraud Detection with Decision Trees"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# Step 1: Create a Synthetic Dataset and Save as CSV
def create_dataset(n_samples=1000, filename="fraud_detection.csv"):
    np.random.seed(42)
    data = {
        'Transaction_ID': np.arange(1, n_samples + 1),
        'Amount': np.random.uniform(10, 1000, n_samples),
        'Type': np.random.choice(['Credit', 'Debit'], n_samples),
        'Is_Fraud': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])  # 5% fraud cases
    }
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"Dataset saved as {filename}")

# Step 2: Load & Preprocess the Dataset
def load_and_preprocess_data(filename="fraud_detection.csv"):
    df = pd.read_csv(filename)
    print("Dataset Loaded Successfully!\n")

    # Handle Missing Values
    print(f"Missing Values:\n{df.isnull().sum()}\n")

    # Drop Transaction_ID as it's not useful for training
    df.drop(columns=['Transaction_ID'], inplace=True)

    # Encode categorical variable 'Type'
    label_encoder = LabelEncoder()
    df['Type'] = label_encoder.fit_transform(df['Type'])  # Credit=0, Debit=1
    return df

# Step 3: Feature Engineering
def feature_engineering(df):
    df['Log_Amount'] = np.log(df['Amount'] + 1)  # Log transformation to reduce skewness
    return df

# Step 4: Train a Decision Tree Classifier
def train_model(X_train, y_train):
    model = DecisionTreeClassifier(random_state=42, max_depth=5)
    model.fit(X_train, y_train)
    return model

# Step 5: Evaluate Model Performance
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print("\nModel Performance:\n")
    print(classification_report(y_test, y_pred))

# Main function to execute all steps
def main():
    # Step 1: Create and Save Dataset
    create_dataset()

    # Step 2: Load & Preprocess Data
    df = load_and_preprocess_data()

    # Step 3: Feature Engineering
    df = feature_engineering(df)

    # Step 4: Prepare Training and Testing Data
    X = df.drop(columns=['Is_Fraud'])
    y = df['Is_Fraud']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Step 5: Train Model
    model = train_model(X_train, y_train)

    # Step 6: Evaluate Model
    evaluate_model(model, X_test, y_test)

if __name__ == "__main__":
    main()